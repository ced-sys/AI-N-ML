{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyNsw0mjatUTxOH7Wb89gZRC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cedamusk/AI-N-ML/blob/main/Ridge_and_Lasso_regression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install Python Libraries\n",
        "1. **Pandas**: A library for data manipulation and analysis. It provides data structures like `DataFrame` and functions to clean, aggregrate, and process data.\n",
        "2. **numpy**: A library for numerical computations. it supports large, multi-dimensional arrays and matrices, along wit a collection of mathematical functions.\n",
        "3. **scikit-learn**: A machine learning library that provides simple and efficient tools for data mining, analysis and modelling, including classification, regression, clustering and dimensionality reduction.\n",
        "4. **matplotlib**: A plotting library for creating static, animated, and interactive visualizations in Python.\n",
        "5. **seaborn**: A statistical data visualization library built on top of Matplotlib, providing a high-level interface for creating attractive and informative graphics."
      ],
      "metadata": {
        "id": "D-smWazaNzKT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R8n331r2gxhy"
      },
      "outputs": [],
      "source": [
        "!pip install pandas numpy scikit-learn matplotlib seaborn"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import Libraries\n",
        "This cell imports various libraries and modules that are essential for data manipulation, machine learning, preprocessing, evaluation and visualiation."
      ],
      "metadata": {
        "id": "ihCSqsT-PICN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n"
      ],
      "metadata": {
        "id": "rugWb_0Kg9sn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prepare Data\n",
        "This function is designed to preprocess a dataset for machine learning tasks. It performs feature-target separation, data splitting, and feature scaling.\n",
        "\n",
        "**Step-by-step Breakdown**:\n",
        "1. **Input**: `df` A pandas Dataframe containing the dataset.\n",
        "2. **Feature-target separation**: `X=df.drop(['Y', 'Year'], axis=1)` Drops the columns `Y`(the target variable) and `Year` (likely irrelevant or categorical data) to create the feature matrix\n",
        "\n",
        "  `X`. `y=df['Y]`: Selects the column `Y` as the target variable.\n",
        "\n",
        "3. **Data-Splitting**: `train_test_split`: Splits the data into training and testing sets: `X_train` and `y_train` for training. `X_test` and `y_test` for testing. `test_size=0.2` allocates 20% of the data for tesing. `random_state=42` ensures reproducibility of the split.\n",
        "\n",
        "4. **Feature Scaling**: `scaler=StandardScaler()`: Creates an instance of the `StandardScaler` for standardizing features by removing the mean and scaling to unit variance.\n",
        "\n",
        "5. **Output**: Returns: `X_train_scaled`: Scaled training feature set. `X_test_scaled`: Scaled testing feature set. `y_train`: Training target values. `y_test`: Testing target values. `scaler` the fitted scaler instance (useful for scaling new data later)."
      ],
      "metadata": {
        "id": "p3CmmW3iQqVh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_data(df):\n",
        "  X=df.drop(['Y', 'Year'], axis=1)\n",
        "  y=df['Y']\n",
        "\n",
        "  X_train, X_test, y_train, y_test=train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "  scaler=StandardScaler()\n",
        "  X_train_scaled=scaler.fit_transform(X_train)\n",
        "  X_test_scaled=scaler.transform(X_test)\n",
        "\n",
        "  return X_train_scaled, X_test_scaled, y_train, y_test, scaler"
      ],
      "metadata": {
        "id": "21UXu3lzhvi1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train and Evaluate models\n",
        "This code defines a Python function, `train_and_evaluate_models` to train and evaluate three regression models. **Linear Regression**, **Ridge Regression**, and **Lasso Regression**.\n",
        "\n",
        "**Input Parameters**:\n",
        "`X_train`: training features\n",
        "`X_test`: Testing features\n",
        "`y_train`: Training target values\n",
        "`y_test`: Testing target values.\n",
        "\n",
        "**Steps in the Function**:\n",
        "1. **Model Initialization**: A dictionary `models` is created containing three regression models: Linear Regression, Ridge Regression (with `alpha=1.0` as the regulaization parameter) and Lassos Regression (with `alpha=1.0` as the regularization parameter).\n",
        "\n",
        "2. **Evaluation results storage**: An empty dictionary `results` is initialized t store the performance metrics for each model.\n",
        "\n",
        "3. **Model Training and Evaluation**: The function iterates through each model in the `models` dictionary. Each model is trained using `model.fit(X_train, y_train)`. Predictions are made for both training (`y_train_pred`) and testing data (`y_test_pred`). Performance metrics are computed **Mean Squared Error (MSE)** for training and testing. **Root Mean Squared Error(RMSE)** for training (square root of MSE). **R^2 Score** for training and testing, **Cross-Validation** scores using 5-fold cross-validation. Mean and Standard deviation of R^2 scores across folds. The results for the model are stored in the `results` dictionary, keyed by the model's name.\n",
        "\n",
        "4. **Return value**: The function returns the `results` dictionary containing; the trained model, predictions for training and testing datasets, RMSE, R^2 scores, and cross-validation statistics (mean and standard deviation of scores)\n"
      ],
      "metadata": {
        "id": "rDaLaLcwlaFo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_and_evaluate_models(X_train, X_test, y_train, y_test):\n",
        "  models={\n",
        "      'Linear Regression': LinearRegression(),\n",
        "      'Ridge Regression': Ridge (alpha=1.0),\n",
        "      \"Lasso Regression\": Lasso(alpha=1.0)\n",
        "  }\n",
        "\n",
        "  results={}\n",
        "\n",
        "  for name, model in models.items():\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    y_train_pred=model.predict(X_train)\n",
        "    y_test_pred=model.predict(X_test)\n",
        "\n",
        "    train_mse=mean_squared_error(y_train, y_train_pred)\n",
        "    test_mse=mean_squared_error(y_test, y_test_pred)\n",
        "    train_r2=r2_score(y_train, y_train_pred)\n",
        "    test_r2=r2_score(y_test, y_test_pred)\n",
        "\n",
        "    cv_scores=cross_val_score(model, X_train, y_train, cv=5, scoring='r2')\n",
        "\n",
        "    results[name]={\n",
        "        'model':model,\n",
        "        'train_predictions': y_train_pred,\n",
        "        'test_predictions': y_test_pred,\n",
        "        'train_rmse': np.sqrt(train_mse),\n",
        "        'train_r2': train_r2,\n",
        "        'test_r2': test_r2,\n",
        "        'cv_scores_mean':cv_scores.mean(),\n",
        "        'cv_scores_std': cv_scores.std()\n",
        "    }\n",
        "\n",
        "  return results"
      ],
      "metadata": {
        "id": "7ZGunLEyigiC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Feature Importance\n",
        "The `plot_feature_importance` function is used to visualize the importance of features in a regression model.\n",
        "\n",
        "###Functionality\n",
        "1. **Input Parameters**\n",
        "\n",
        "  `model`: A trained regression model witha `coef_` attribute (e.g., Linear Regression, Ridge or Lasso)\n",
        "\n",
        "  `feature_names`: A list or array containing the names of the features corresponding to the model's coefficient.\n",
        "\n",
        "2. **feature importance DataFrame**:\n",
        "  The function create a Pandas DataFrame named `importance` with:\n",
        "    \n",
        "    `feature`: Names of the features.\n",
        "    `coefficient`: Absolute values of the coefficients (`np.abs(model.coef_)`), representing the magnitude of the feature's contribution to the prediction.\n",
        "\n",
        "3. **Sorting by Importance**: The features are sorted by the absolute value of their coefficients in descending order (`ascending=False`).\n",
        "\n",
        "4. **Bar Plot**: A bar plot is created using `matplotlib` to visualize the feature importance:\n",
        "\n",
        "    X-axis: feature names\n",
        "    Y-axis: Absolute coefficient values.\n",
        "    the plot is formatted with labels, a rotated X-axis for readability and a title.\n",
        "\n",
        "5. **Display**: The `plt.show()` function is used to display the plot."
      ],
      "metadata": {
        "id": "afieDS-ModOH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_feature_importance(model, feature_names):\n",
        "  importance=pd.DataFrame({\n",
        "      'feature': feature_names,\n",
        "      'coefficient': np.abs(model.coef_)\n",
        "  })\n",
        "\n",
        "  importance=importance.sort_values('coefficient', ascending=False)\n",
        "\n",
        "  plt.figure(figsize=(12, 6))\n",
        "  plt.bar(importance['feature'], importance['coefficient'])\n",
        "  plt.xticks(rotation=45)\n",
        "  plt.title('Feature Importance')\n",
        "  plt.xlabel('Features')\n",
        "  plt.ylabel(\"Absolute Coefficient Value\")\n",
        "  plt.tight_layout()\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "9Hsp_j7vkoJe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_predictions_vs_actual(results, y_train, y_test):\n",
        "  fig, axes=plt.subplots(len(results), 2, figsize=(15, 5*len(results)))\n",
        "  fig.suptitle('Predicted Vs Actual Values for All Models', y=1.02, fontsize=16)\n",
        "\n",
        "  for i, (name, metrics) in enumerate(results.items()):\n",
        "    axes[i, 0].scatter(y_train, metrics['train_predictions'], alpha=0.5)\n",
        "    axes[i, 0].plot([y_train.min(), y_train.max()], [y_train.min(), y_train.max()], 'r--', lw=2)\n",
        "    axes[i, 0].set_title(f'{name}-Training Set')\n",
        "    axes[i, 0].set_xlabel(\"Actual Values\")\n",
        "    axes[i, 0].set_ylabel('Predicted Values')\n",
        "\n",
        "    axes[i,1].scatter(y_test, metrics['test_predictions'], alpha=0.5)\n",
        "    axes[i,1].plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)\n",
        "    axes[i,1].set_title(f'{name}-Test Set')\n",
        "    axes[i,1].set_xlabel(\"Actual Values\")\n",
        "    axes[i, 1].set_ylabel(\"Predicted Values\")\n",
        "\n",
        "  plt.tight_layout()\n",
        "  plt.show()\n"
      ],
      "metadata": {
        "id": "gOWtOS2jlfDl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Predictions Vs Actual\n",
        "The `plot_predictions_vs_actual` function generates visualizations to compare actual vs. predicted values for both the training and testing datasets across multiple regression models.\n",
        "\n",
        "###Functionality\n",
        "1. **Input parameters**:\n",
        "\n",
        "    `results`: A dictionary containig model evaluation results. Each entry corresponds to a model and includes `train_predictions` and `test_predictions`.\n",
        "\n",
        "    `y_train`: the actual target values for training dataset.\n",
        "\n",
        "    `y_test`: the actual target values for the testing dataset.\n",
        "\n",
        "2. **Figure and Subplots**:\n",
        "\n",
        "    A `matplotlib` figure with sublots is created. Rows correspond to the number of models in `results`. Each model has two columns Column1: Training set (actual vs. predicted values). Column2: testing set (actual vs. predicted values).\n",
        "\n",
        "3. **Scatter plots**: for each model, two scatter plots are created:\n",
        "\n",
        "  **Training set**:`y_train` (actual) vs. `metrics['train_predictions']` (predicted).\n",
        "\n",
        "  **Testing set**: `y_test` (actual) vs. `metrics['test_predictions']` (predicted).\n",
        "\n",
        "  A reference line (`r--`, dashed red) is plotted on both scatter plots to represent the ideal case where predicted values perfectly match actual values.\n",
        "\n",
        "4. **Title, labels and layout**: Each plot is titled with the model's name and dataset type (Training/Test). Axes are labelled for clarity. The layout is adjusted using `plt.tight_layout()` to ensure proper spacing.\n",
        "\n",
        "5. **Display**: The plots are displayed using `plt.show()`."
      ],
      "metadata": {
        "id": "3Oj-v0iurQXD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_residuals(results, y_train, y_test):\n",
        "  fig, axes=plt.subplots(len(results), 2, figsize=(15, 5*len(results)))\n",
        "  fig.suptitle('residual Plots for All models', y=1.02, fontsize=16)\n",
        "\n",
        "  for i, (name, metrics) in enumerate(results.items()):\n",
        "\n",
        "    train_residuals=y_train-metrics['train_predictions']\n",
        "    axes[i, 0].scatter(metrics['train_predictions'], train_residuals, alpha=0.5)\n",
        "    axes[i, 0].axhline(y=0, color='r', linestyle='--')\n",
        "    axes[i, 0].set_title(f'{name}-Trainig Set residuals')\n",
        "    axes[i, 0].set_xlabel('Predicted Values')\n",
        "    axes[i, 0].set_ylabel(\"Residuals\")\n",
        "\n",
        "    test_residuals=y_test-metrics['test_predictions']\n",
        "    axes[i, 1].scatter(metrics['test_predictions'], test_residuals, alpha=0.5)\n",
        "    axes[i, 1].axhline(y=0, color='r', linestyle='--')\n",
        "    axes[i, 1].set_title(f'{name}-Test set Residuals')\n",
        "    axes[i, 1].set_xlabel('Predicted Values')\n",
        "    axes[i, 1].set_ylabel('Residuals')\n",
        "\n",
        "  plt.tight_layout()\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "mohFaHdVnl60"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Error Distribution\n",
        "The `plot_error_distribution` function generates histograms of predictions errors for both the training and testing datasets across multiple regression models. This visualization provides insights into how well the models are predicting and whether there are systematic biases or outliers in the pedictions.\n",
        "\n",
        "###Functionality\n",
        "1. **Input Parameters**:\n",
        "\n",
        "    `results`: A dictionary containing evaluation results for multiple models. Each entry includes `train_predictions` and `test_predictions`.\n",
        "\n",
        "    `y_train`: The actual target values for the training dataset.\n",
        "\n",
        "    `y_test`: The actual target values for the testing dataset.\n",
        "\n",
        "2. **Figure and Subplots**:\n",
        "\n",
        "  A `matplotlib` figure with subplots is created:\n",
        "    \n",
        "      Rows correspond to the numner of models in `results`. Each model has two columns:\n",
        "        Column 1: Training set error distribution\n",
        "        Column 2: Testing set error distribution\n",
        "\n",
        "3. **Error Calculation**:\n",
        "\n",
        "  For each model:\n",
        "\n",
        "      Training errors: Error=(y_train)-(train_predictions)\n",
        "\n",
        "      Testing Errors: Error= (y_test)-(test_predictions).\n",
        "\n",
        "\n",
        "4. **Error distribution plots**:\n",
        "\n",
        "Histograms are plotted for both training and testing errors using `sns.histplot`(from the Seaborn Library).\n",
        "\n",
        "Kernel Density Estimation (KDE) is enabled (`kde=True`) to visualize the underlying distribution smoothly.\n",
        "\n",
        "5. **Titles, labels and layout**:\n",
        "  Each plot is filled with the model's name and dataset type (Training/Test). X-axis is labelled as the \"Prediction Error\". Layout is adjusted using `plt.tight_layout()` for better spacing.\n",
        "\n",
        "6. **Display**:\n",
        "The plots are displayed using `plt.show()`."
      ],
      "metadata": {
        "id": "xZIjR8mUVFzW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "def plot_error_distribution(results, y_train, y_test):\n",
        "  fig, axes=plt.subplots(len(results), 2, figsize=(15, 5*len(results)))\n",
        "  fig.suptitle('Error Distribution for All models', y=1.02, fontsize=16)\n",
        "\n",
        "  for i, (name, metrics) in enumerate(results.items()):\n",
        "    train_errors=y_train-metrics['train_predictions']\n",
        "    sns.histplot(train_errors, kde=True, ax=axes[i, 0])\n",
        "    axes[i, 0].set_title(f'{name}- Training Error distribution')\n",
        "    axes[i, 0].set_xlabel('Prediction Error')\n",
        "\n",
        "    test_errors=y_test-metrics['test_predictions']\n",
        "    sns.histplot(test_errors, kde=True, ax=axes[i, 1])\n",
        "    axes[i, 1].set_title(f'{name}- Test Error Distribution')\n",
        "    axes[i, 1].set_xlabel('Prediction Error')\n",
        "\n",
        "  plt.tight_layout()\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "0gISVA6ypXnk",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Ridge and Lasso Regression\n",
        "This script combines the previous functions to perform end-to-end data preparation, model training, evaluation, and visualization for a regression task."
      ],
      "metadata": {
        "id": "Qz_MtIBDZ2Np"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df=pd.read_csv('synthetic_ridge_lasso_data.csv')\n",
        "X_train, X_test, y_train, y_test, scaler=prepare_data(df)\n",
        "\n",
        "results=train_and_evaluate_models(X_train, X_test, y_train, y_test)\n",
        "\n",
        "for name, metrics in results.items():\n",
        "  print(f\"\\n{name} Results:\")\n",
        "  print(f\"Training RMSE: {metrics['train_rmse']:.4f}\")\n",
        "  print(f\"Training R2 Score: {metrics['train_r2']:.4f}\")\n",
        "  print(f\"Test R2 Score: {metrics['test_r2']:.4f}\")\n",
        "  print(f\"Cross-validation R2(mean +/- std): {metrics['cv_scores_mean']:.4f}+/-{metrics['cv_scores_std']:.4f}\")\n",
        "\n",
        "  plot_feature_importance(results['Linear Regression']['model'], df.drop([\"Y\", \"Year\"], axis=1).columns)\n",
        "  plot_predictions_vs_actual(results, y_train, y_test)\n",
        "  plot_residuals(results, y_train, y_test)\n",
        "  plot_error_distribution(results, y_train, y_test)"
      ],
      "metadata": {
        "id": "H9pEgDUYqxim"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}